{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4afcb1-d2fa-4164-80bd-23456c7673a7",
   "metadata": {},
   "source": [
    "#¬†Rossman dataset \n",
    "\n",
    "Fuente catboost: https://catboost.ai/docs/en/\n",
    "\n",
    "Correr esto:\n",
    "\n",
    "!pip install xgboost==1.7.6 scikit-learn==1.2.2\n",
    "\n",
    "https://www.kaggle.com/competitions/rossmann-store-sales/data\n",
    "\n",
    "- Id - an Id that represents a (Store, Date) duple within the test set\n",
    "- Store - a unique Id for each store\n",
    "- Sales - the turnover for any given day (this is what you are predicting)\n",
    "- Customers - the number of customers on a given day\n",
    "- Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "- StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "- SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "- Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
    "- CompetitionDistance - distance in meters to the nearest competitor store\n",
    "- CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
    "- Promo - indicates whether a store is running a promo on that day\n",
    "- Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
    "- Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
    "- PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8bc43-d30b-45ea-a3a5-8f8eb6d664b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tr/mb4tgd2j63j6tf50s0kl_p680000gn/T/ipykernel_91493/2064280237.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('Data/rossman.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# CARGA Y PREPROCESAMIENTO\n",
    "# ----------------------------\n",
    "# Carga de datos\n",
    "train = pd.read_csv('Data/rossman.csv')\n",
    "stores = pd.read_csv('Data/store.csv')\n",
    "\n",
    "# Merge de ambos datasets\n",
    "df = pd.merge(train, stores, on='Store')\n",
    "\n",
    "# Convertir la columna de fecha\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Ordenar por fecha para evitar leakage\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Filtrar solo tiendas abiertas\n",
    "df = df[df['Open'] == 1]\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# ----------------------------\n",
    "\n",
    "# Variables temporales √∫tiles\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Eliminar columnas que no usaremos\n",
    "df = df.drop(columns=['Date', 'Store', 'Customers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb33e22-f883-4bb7-bd28-a24e2fb525bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# SEPARACI√ìN TEMPORAL: 90% pasado, 20% futuro\n",
    "# ----------------------------\n",
    "split_index = int(len(df) * 0.9)\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "\n",
    "# Separar X y y\n",
    "target = 'Sales'\n",
    "X_train = train_df.drop(columns=target)\n",
    "y_train = train_df[target]\n",
    "X_test = test_df.drop(columns=target)\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Alinear columnas en test con train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca96f18-d67a-48c6-a16f-f0ae2ddaa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Detectar columnas categ√≥ricas (n√∫mero bajo de categor√≠as)\n",
    "cat_cols = [col for col in X_train.columns if X_train[col].nunique() < 50]\n",
    "\n",
    "# Paso 3: Forzar columnas categ√≥ricas a string y llenar nulos\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype(str).fillna('missing')\n",
    "    X_test[col] = X_test[col].astype(str).fillna('missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e89c3-f819-4dc2-a3bb-49ff8c00dfec",
   "metadata": {},
   "source": [
    "### üê± Tuneando CatBoost\n",
    "\n",
    "- **`iterations`**:  \n",
    "  N√∫mero total de √°rboles a entrenar. Si usas `early_stopping_rounds`, puedes poner un n√∫mero alto sin preocuparte por overfitting.\n",
    "\n",
    "- **`depth`**:  \n",
    "  Profundidad m√°xima de cada √°rbol. Profundidades mayores capturan m√°s complejidad, pero pueden sobreajustar.\n",
    "\n",
    "- **`learning_rate`**:  \n",
    "  Qu√© tan r√°pido aprende el modelo. Valores m√°s bajos requieren m√°s iteraciones, pero suelen generalizar mejor.\n",
    "\n",
    "- **`subsample`**:  \n",
    "  Fracci√≥n de observaciones usadas para entrenar cada √°rbol. Se controla mediante `bootstrap_type` + `subsample`.\n",
    "\n",
    "- **`rsm`** (Random Subspace Method):  \n",
    "  Fracci√≥n de columnas (features) usadas en cada split. Equivalente a `colsample_bytree`.\n",
    "\n",
    "- **`early_stopping_rounds`**:  \n",
    "  Detiene el entrenamiento si la m√©trica en el set de validaci√≥n no mejora en N iteraciones. Se activa al pasar `eval_set`.\n",
    "\n",
    "- **`eval_metric`**:  \n",
    "  M√©trica usada durante entrenamiento para monitorear desempe√±o (ej: `'RMSE'`, `'MAE'`, `'Logloss'`, `'AUC'`). Define si se activa early stopping.\n",
    "\n",
    "- **`min_data_in_leaf`**:  \n",
    "  N√∫mero m√≠nimo de muestras requeridas para hacer un split. Sirve para evitar sobreajuste (similar a `min_child_samples` en LightGBM).\n",
    "\n",
    "- **`l2_leaf_reg`**:  \n",
    "  Regularizaci√≥n L2 (Ridge) aplicada a los pesos de las hojas. Ayuda a controlar complejidad del modelo.\n",
    "\n",
    "- **`random_strength`**:  \n",
    "  Regularizaci√≥n de los splits. Cuanto m√°s alto, m√°s aleatoriedad al decidir los splits ‚Üí √∫til contra overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6d4fda-03c5-42e9-aca2-f81127e45711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0435957\ttest: 0.0306309\tbest: 0.0306309 (0)\ttotal: 190ms\tremaining: 31m 40s\n",
      "100:\tlearn: 0.6134502\ttest: 0.5643274\tbest: 0.5643274 (100)\ttotal: 9.89s\tremaining: 16m 9s\n",
      "200:\tlearn: 0.6983887\ttest: 0.6460173\tbest: 0.6460173 (200)\ttotal: 20.5s\tremaining: 16m 39s\n",
      "300:\tlearn: 0.7424881\ttest: 0.6857417\tbest: 0.6857417 (300)\ttotal: 31.8s\tremaining: 17m 4s\n",
      "400:\tlearn: 0.7703974\ttest: 0.7099062\tbest: 0.7099062 (400)\ttotal: 43.4s\tremaining: 17m 20s\n",
      "500:\tlearn: 0.7915378\ttest: 0.7283160\tbest: 0.7283160 (500)\ttotal: 55.6s\tremaining: 17m 34s\n",
      "600:\tlearn: 0.8057578\ttest: 0.7448941\tbest: 0.7448941 (600)\ttotal: 1m 7s\tremaining: 17m 42s\n",
      "700:\tlearn: 0.8172454\ttest: 0.7544104\tbest: 0.7546981 (696)\ttotal: 1m 20s\tremaining: 17m 43s\n",
      "800:\tlearn: 0.8261818\ttest: 0.7630289\tbest: 0.7630304 (799)\ttotal: 1m 32s\tremaining: 17m 43s\n",
      "900:\tlearn: 0.8341629\ttest: 0.7746682\tbest: 0.7746728 (899)\ttotal: 1m 45s\tremaining: 17m 42s\n",
      "1000:\tlearn: 0.8417151\ttest: 0.7813506\tbest: 0.7813506 (1000)\ttotal: 1m 57s\tremaining: 17m 38s\n",
      "1100:\tlearn: 0.8468962\ttest: 0.7845691\tbest: 0.7848993 (1082)\ttotal: 2m 10s\tremaining: 17m 35s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7848992744\n",
      "bestIteration = 1082\n",
      "\n",
      "Shrink model to first 1083 iterations.\n",
      " R¬≤: 0.7849\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Paso 5: Entrenar el modelo\n",
    "model = CatBoostRegressor(\n",
    "    iterations=10000,           # ‚âà n_estimators\n",
    "    depth=5,                    # ‚âà max_depth\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.5,              # igual que en XGBoost\n",
    "    rsm=0.8,                    # ‚âà colsample_bytree\n",
    "    eval_metric='R2',           # m√©trica para evaluar\n",
    "    l2_leaf_reg=0.1,            # ‚âà reg_lambda\n",
    "    random_strength=5,          # ‚âà gamma (penalizaci√≥n para splits)\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=20    # early stopping con validaci√≥n\n",
    ")\n",
    "\n",
    "# Entrenamiento con eval_set incluyendo train y test\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_cols,\n",
    "    eval_set=[(X_test, y_test)],\n",
    ")\n",
    "\n",
    "# Paso 6: Evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\" R¬≤: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e0235d-9bfe-4dfd-b549-9c854a4bd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    'feature': model.feature_names_,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8221a6-c911-4f5b-bc17-ae173f1c71cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CompetitionDistance</td>\n",
       "      <td>27.583348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promo</td>\n",
       "      <td>13.433849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CompetitionOpenSinceYear</td>\n",
       "      <td>9.550844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>StoreType</td>\n",
       "      <td>8.854410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CompetitionOpenSinceMonth</td>\n",
       "      <td>8.733118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Assortment</td>\n",
       "      <td>6.662655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Promo2SinceWeek</td>\n",
       "      <td>5.742445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Promo2SinceYear</td>\n",
       "      <td>4.789122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DayOfWeek</td>\n",
       "      <td>2.995582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PromoInterval</td>\n",
       "      <td>2.462945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>week_of_year</td>\n",
       "      <td>2.380795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>2.021640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month</td>\n",
       "      <td>1.905384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>day_of_month</td>\n",
       "      <td>1.787948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Promo2</td>\n",
       "      <td>0.553527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0.299144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StateHoliday</td>\n",
       "      <td>0.133656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SchoolHoliday</td>\n",
       "      <td>0.109588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  importance\n",
       "7         CompetitionDistance   27.583348\n",
       "2                       Promo   13.433849\n",
       "9    CompetitionOpenSinceYear    9.550844\n",
       "5                   StoreType    8.854410\n",
       "8   CompetitionOpenSinceMonth    8.733118\n",
       "6                  Assortment    6.662655\n",
       "11            Promo2SinceWeek    5.742445\n",
       "12            Promo2SinceYear    4.789122\n",
       "0                   DayOfWeek    2.995582\n",
       "13              PromoInterval    2.462945\n",
       "17               week_of_year    2.380795\n",
       "15                day_of_week    2.021640\n",
       "14                      month    1.905384\n",
       "16               day_of_month    1.787948\n",
       "10                     Promo2    0.553527\n",
       "18                 is_weekend    0.299144\n",
       "3                StateHoliday    0.133656\n",
       "4               SchoolHoliday    0.109588\n",
       "1                        Open    0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f4c44-a96b-4d77-bddf-baa04c940c86",
   "metadata": {},
   "source": [
    "## Comparativa: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ ¬øCu√°ndo usar cada uno?\n",
    "\n",
    "## ‚úÖ ¬øCu√°ndo usar XGBoost, LightGBM o CatBoost?\n",
    "\n",
    "| Situaci√≥n                                                  | Recomendaci√≥n                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular peque√±o o mediano                          | ‚úÖ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables num√©ricas                 | ‚úÖ LightGBM                                         |\n",
    "| Muchas variables categ√≥ricas sin preprocesamiento          | ‚úÖ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ‚úÖ XGBoost (muy probado en producci√≥n y Kaggle)     |\n",
    "| Entrenamiento r√°pido con buen desempe√±o                    | ‚úÖ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ‚úÖ Cualquiera, pero CatBoost da mejores resultados con categ√≥ricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ‚úÖ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ‚úÖ XGBoost o LightGBM                               |\n",
    "| Tuning autom√°tico (Optuna, GridSearchCV, etc.)             | ‚úÖ LightGBM (r√°pido y convergente)                  |\n",
    "| Producci√≥n en sistemas legacy o APIs bien documentadas     | ‚úÖ XGBoost (mayor madurez, m√°s integraci√≥n)         |\n",
    "| Clasificaci√≥n multi-label o problemas no est√°ndar          | ‚úÖ XGBoost (soporte m√°s flexible)                   |\n",
    "\n",
    "\n",
    "## üß† Tips\n",
    "\n",
    "- **LightGBM** puede overfittear f√°cilmente ‚Üí cuida `num_leaves` y `min_data_in_leaf`.\n",
    "- **CatBoost** funciona muy bien con defaults y sin preprocessing.\n",
    "- **XGBoost** es muy robusto y balanceado, ideal si ya tienes un pipeline con encoding hecho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45222ede-2898-40df-811e-fafd41912684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f0d61-fcf9-48b2-af1c-ff7685862998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
