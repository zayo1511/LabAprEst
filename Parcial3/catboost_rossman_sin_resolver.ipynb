{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4afcb1-d2fa-4164-80bd-23456c7673a7",
   "metadata": {},
   "source": [
    "# Rossman dataset \n",
    "\n",
    "Fuente catboost: https://catboost.ai/docs/en/\n",
    "\n",
    "Correr esto:\n",
    "\n",
    "!pip install xgboost==1.7.6 scikit-learn==1.2.2\n",
    "\n",
    "https://www.kaggle.com/competitions/rossmann-store-sales/data\n",
    "\n",
    "- Id - an Id that represents a (Store, Date) duple within the test set\n",
    "- Store - a unique Id for each store\n",
    "- Sales - the turnover for any given day (this is what you are predicting)\n",
    "- Customers - the number of customers on a given day\n",
    "- Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "- StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "- SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "- Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
    "- CompetitionDistance - distance in meters to the nearest competitor store\n",
    "- CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
    "- Promo - indicates whether a store is running a promo on that day\n",
    "- Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
    "- Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
    "- PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8bc43-d30b-45ea-a3a5-8f8eb6d664b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/z6bgw1fj4vng6rs97g1chhz00000gn/T/ipykernel_11112/2870048983.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('rossman.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# CARGA Y PREPROCESAMIENTO\n",
    "# ----------------------------\n",
    "# Carga de datos\n",
    "train = pd.read_csv('rossman.csv')\n",
    "stores = pd.read_csv('store.csv')\n",
    "\n",
    "# Merge de ambos datasets\n",
    "df = pd.merge(train, stores, on='Store')\n",
    "\n",
    "# Convertir la columna de fecha\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Ordenar por fecha para evitar leakage\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Filtrar solo tiendas abiertas\n",
    "df = df[df['Open'] == 1]\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# ----------------------------\n",
    "\n",
    "# Variables temporales útiles\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Eliminar columnas que no usaremos\n",
    "df = df.drop(columns=['Date', 'Store', 'Customers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb33e22-f883-4bb7-bd28-a24e2fb525bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# SEPARACIÓN TEMPORAL: 90% pasado, 20% futuro\n",
    "# ----------------------------\n",
    "split_index = int(len(df) * 0.9)\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "\n",
    "# Separar X y y\n",
    "target = 'Sales'\n",
    "X_train = train_df.drop(columns=target)\n",
    "y_train = train_df[target]\n",
    "X_test = test_df.drop(columns=target)\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Alinear columnas en test con train\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca96f18-d67a-48c6-a16f-f0ae2ddaa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Detectar columnas categóricas (número bajo de categorías)\n",
    "cat_cols = [col for col in X_train.columns if X_train[col].nunique() < 50]\n",
    "\n",
    "# Paso 3: Forzar columnas categóricas a string y llenar nulos\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype(str).fillna('missing')\n",
    "    X_test[col] = X_test[col].astype(str).fillna('missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e89c3-f819-4dc2-a3bb-49ff8c00dfec",
   "metadata": {},
   "source": [
    "### 🐱 Tuneando CatBoost\n",
    "\n",
    "- **`iterations`**:  \n",
    "  Número total de árboles a entrenar. Si usas `early_stopping_rounds`, puedes poner un número alto sin preocuparte por overfitting.\n",
    "\n",
    "- **`depth`**:  \n",
    "  Profundidad máxima de cada árbol. Profundidades mayores capturan más complejidad, pero pueden sobreajustar.\n",
    "\n",
    "- **`learning_rate`**:  \n",
    "  Qué tan rápido aprende el modelo. Valores más bajos requieren más iteraciones, pero suelen generalizar mejor.\n",
    "\n",
    "- **`subsample`**:  \n",
    "  Fracción de observaciones usadas para entrenar cada árbol. Se controla mediante `bootstrap_type` + `subsample`.\n",
    "\n",
    "- **`rsm`** (Random Subspace Method):  \n",
    "  Fracción de columnas (features) usadas en cada split. Equivalente a `colsample_bytree`.\n",
    "\n",
    "- **`early_stopping_rounds`**:  \n",
    "  Detiene el entrenamiento si la métrica en el set de validación no mejora en N iteraciones. Se activa al pasar `eval_set`.\n",
    "\n",
    "- **`eval_metric`**:  \n",
    "  Métrica usada durante entrenamiento para monitorear desempeño (ej: `'RMSE'`, `'MAE'`, `'Logloss'`, `'AUC'`). Define si se activa early stopping.\n",
    "\n",
    "- **`min_data_in_leaf`**:  \n",
    "  Número mínimo de muestras requeridas para hacer un split. Sirve para evitar sobreajuste (similar a `min_child_samples` en LightGBM).\n",
    "\n",
    "- **`l2_leaf_reg`**:  \n",
    "  Regularización L2 (Ridge) aplicada a los pesos de las hojas. Ayuda a controlar complejidad del modelo.\n",
    "\n",
    "- **`random_strength`**:  \n",
    "  Regularización de los splits. Cuanto más alto, más aleatoriedad al decidir los splits → útil contra overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6d4fda-03c5-42e9-aca2-f81127e45711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0435957\ttest: 0.0306309\tbest: 0.0306309 (0)\ttotal: 259ms\tremaining: 43m 7s\n",
      "100:\tlearn: 0.6134502\ttest: 0.5643274\tbest: 0.5643274 (100)\ttotal: 17.6s\tremaining: 28m 49s\n",
      "200:\tlearn: 0.6983887\ttest: 0.6460173\tbest: 0.6460173 (200)\ttotal: 38.6s\tremaining: 31m 20s\n",
      "300:\tlearn: 0.7424881\ttest: 0.6857417\tbest: 0.6857417 (300)\ttotal: 55.4s\tremaining: 29m 44s\n",
      "400:\tlearn: 0.7703974\ttest: 0.7099062\tbest: 0.7099062 (400)\ttotal: 1m 13s\tremaining: 29m 14s\n",
      "500:\tlearn: 0.7915378\ttest: 0.7283160\tbest: 0.7283160 (500)\ttotal: 1m 30s\tremaining: 28m 28s\n",
      "600:\tlearn: 0.8055262\ttest: 0.7426717\tbest: 0.7426929 (595)\ttotal: 1m 46s\tremaining: 27m 49s\n",
      "700:\tlearn: 0.8164837\ttest: 0.7524627\tbest: 0.7524627 (700)\ttotal: 2m 3s\tremaining: 27m 12s\n",
      "800:\tlearn: 0.8258509\ttest: 0.7596249\tbest: 0.7596249 (800)\ttotal: 2m 19s\tremaining: 26m 41s\n",
      "900:\tlearn: 0.8349134\ttest: 0.7700189\tbest: 0.7700256 (899)\ttotal: 2m 35s\tremaining: 26m 15s\n",
      "1000:\tlearn: 0.8412016\ttest: 0.7773651\tbest: 0.7774170 (997)\ttotal: 2m 53s\tremaining: 26m 1s\n",
      "1100:\tlearn: 0.8466986\ttest: 0.7848495\tbest: 0.7848495 (1100)\ttotal: 3m 10s\tremaining: 25m 43s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7899889352\n",
      "bestIteration = 1167\n",
      "\n",
      "Shrink model to first 1168 iterations.\n",
      " R²: 0.7900\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Paso 5: Entrenar el modelo\n",
    "model = CatBoostRegressor(\n",
    "    iterations=10000,           # ≈ n_estimators\n",
    "    depth=5,                    # ≈ max_depth\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.5,              # igual que en XGBoost\n",
    "    rsm=0.8,                    # ≈ colsample_bytree\n",
    "    eval_metric='R2',           # métrica para evaluar\n",
    "    l2_leaf_reg=0.1,            # ≈ reg_lambda\n",
    "    random_strength=5,          # ≈ gamma (penalización para splits)\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=20    # early stopping con validación\n",
    ")\n",
    "\n",
    "# Entrenamiento con eval_set incluyendo train y test\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_cols,\n",
    "    eval_set=[(X_test, y_test)],\n",
    ")\n",
    "\n",
    "# Paso 6: Evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\" R²: {r2_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e0235d-9bfe-4dfd-b549-9c854a4bd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    'feature': model.feature_names_,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8221a6-c911-4f5b-bc17-ae173f1c71cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CompetitionDistance</td>\n",
       "      <td>27.545098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promo</td>\n",
       "      <td>13.426763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CompetitionOpenSinceYear</td>\n",
       "      <td>9.705607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>StoreType</td>\n",
       "      <td>8.803276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CompetitionOpenSinceMonth</td>\n",
       "      <td>8.745706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Assortment</td>\n",
       "      <td>6.595940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Promo2SinceWeek</td>\n",
       "      <td>5.663758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Promo2SinceYear</td>\n",
       "      <td>4.800404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DayOfWeek</td>\n",
       "      <td>3.035441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PromoInterval</td>\n",
       "      <td>2.551090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>week_of_year</td>\n",
       "      <td>2.388790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>1.994292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month</td>\n",
       "      <td>1.900215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>day_of_month</td>\n",
       "      <td>1.799450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Promo2</td>\n",
       "      <td>0.498869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0.300249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StateHoliday</td>\n",
       "      <td>0.135238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SchoolHoliday</td>\n",
       "      <td>0.109814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  importance\n",
       "7         CompetitionDistance   27.545098\n",
       "2                       Promo   13.426763\n",
       "9    CompetitionOpenSinceYear    9.705607\n",
       "5                   StoreType    8.803276\n",
       "8   CompetitionOpenSinceMonth    8.745706\n",
       "6                  Assortment    6.595940\n",
       "11            Promo2SinceWeek    5.663758\n",
       "12            Promo2SinceYear    4.800404\n",
       "0                   DayOfWeek    3.035441\n",
       "13              PromoInterval    2.551090\n",
       "17               week_of_year    2.388790\n",
       "15                day_of_week    1.994292\n",
       "14                      month    1.900215\n",
       "16               day_of_month    1.799450\n",
       "10                     Promo2    0.498869\n",
       "18                 is_weekend    0.300249\n",
       "3                StateHoliday    0.135238\n",
       "4               SchoolHoliday    0.109814\n",
       "1                        Open    0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f4c44-a96b-4d77-bddf-baa04c940c86",
   "metadata": {},
   "source": [
    "## Comparativa: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "| Característica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | Rápido, pero más lento que LightGBM y CatBoost             | 🔥 Muy rápido gracias a histogramas y leaf-wise growth      | Rápido, aunque un poco más lento que LightGBM                  |\n",
    "| **Precisión**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categóricas                        |\n",
    "| **Variables categóricas**   | ❌ No las maneja (requiere encoding manual)                | ❌ No las maneja (requiere encoding manual)                 | ✅ Soporte nativo + regularización secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ✅ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ✅ Automático                                               | ✅ Automático                                                | ✅ Automático                                                   |\n",
    "| **Soporte GPU**             | ✅ Sí (bastante estable)                                   | ✅ Sí (muy rápido)                                           | ✅ Sí (algo más limitado)                                      |\n",
    "| **Instalación**             | Fácil (`pip install xgboost`)                             | Fácil (`pip install lightgbm`)                              | Un poco más pesada (`pip install catboost`)                   |\n",
    "| **Documentación**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacción con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ✅ Neutral                                                  | ✅ Neutral                                                   | ⚠️ Sensible (por codificación secuencial)                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ ¿Cuándo usar cada uno?\n",
    "\n",
    "## ✅ ¿Cuándo usar XGBoost, LightGBM o CatBoost?\n",
    "\n",
    "| Situación                                                  | Recomendación                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular pequeño o mediano                          | ✅ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables numéricas                 | ✅ LightGBM                                         |\n",
    "| Muchas variables categóricas sin preprocesamiento          | ✅ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ✅ XGBoost (muy probado en producción y Kaggle)     |\n",
    "| Entrenamiento rápido con buen desempeño                    | ✅ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ✅ Cualquiera, pero CatBoost da mejores resultados con categóricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ✅ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ✅ XGBoost o LightGBM                               |\n",
    "| Tuning automático (Optuna, GridSearchCV, etc.)             | ✅ LightGBM (rápido y convergente)                  |\n",
    "| Producción en sistemas legacy o APIs bien documentadas     | ✅ XGBoost (mayor madurez, más integración)         |\n",
    "| Clasificación multi-label o problemas no estándar          | ✅ XGBoost (soporte más flexible)                   |\n",
    "\n",
    "\n",
    "## 🧠 Tips\n",
    "\n",
    "- **LightGBM** puede overfittear fácilmente → cuida `num_leaves` y `min_data_in_leaf`.\n",
    "- **CatBoost** funciona muy bien con defaults y sin preprocessing.\n",
    "- **XGBoost** es muy robusto y balanceado, ideal si ya tienes un pipeline con encoding hecho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45222ede-2898-40df-811e-fafd41912684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f0d61-fcf9-48b2-af1c-ff7685862998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
