{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbol de decisi√≥n\n",
    "\n",
    "Un √°rbol de decisi√≥n es un modelo de predicci√≥n, el cual utiliza distinos niveles de clasificaci√≥n de datos con base en cierto valor (umbral). Divide los datos en grupos que sobrepasan o no ese umbral.\n",
    "\n",
    "## Regresi√≥n\n",
    "\n",
    "Este modelo utiliza todas las varibles y splits posibles, donde finalmente aplica el tipo de split con mayor reducci√≥n de varianza, con base en el error cuadr√°tico medio.\n",
    "\n",
    "$$\\text{Reducci√≥n de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)$$\n",
    "\n",
    "## Clasificaci√≥n\n",
    "\n",
    "De manera similar a la regresi√≥n, el calsificador utiliza todas las variables y umbrales posibles, eligiendo en este caso el umbral que maximice la impureza, con base en Gini o entrop√≠a. \n",
    "\n",
    "$$\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Regresi√≥n\n",
    "\n",
    "Utilizando bootstrap se hacen muestreos aleatorios sobre tus datos, con los cuales se entrena el modelo y se relaiza la predicci√≥n. Al final, cuando se tienen todas las predicciones, se calcula la media de esas predicciones, la cual es la predicci√≥n del random forest.\n",
    "\n",
    "## Clasificaci√≥n\n",
    "\n",
    "En el modelo clasificador, en lugar de calcular la media de todas las predicciones, se utiliza la categor√≠a que m√°s repite (modal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "Es un m√©todo que combina varios modelos \"d√©biles\" como los √°rboles de decisi√≥n, de modo que genera una predicci√≥n inicial y calcula error de la misma, para despues crear un modelo que trate de predecir este error, de modod que logre diminuirlo. Esto se repite iterativamente con un factor de aprendizaje hasta covnverger, muy parecido al decenso en gradiente. \n",
    "\n",
    "La predicci√≥n del √°rbol anterior es el promedio de los valores obtenidos por hoja. La diferencia entre el regresor y el clasificador es que en el clasificador usa los log-odds para predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Es un modelo que consiste en sumar un nuevo √°rbol iterativamente a la predicci√≥n que ya se tiene.\n",
    "\n",
    "$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y}_i^{(t)}$ es la predicci√≥n\n",
    "- $f_t(x_i)$ es el nuevo √°rbol \n",
    "\n",
    "Con cada una de estas iteraciones su busca minimizar la contribuci√≥n marginl del nuevo √°rbol a la p√©rdida total. Esta funci√≥n de p√©rdida consiste de 2 partes: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "}_{\\text{Parte ya construida (constante en esta iteraci√≥n)}} +\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "}_{\\text{Lo que a√±ade el nuevo √°rbol $f_t$}}\n",
    "$$\n",
    "\n",
    "donde $\\Omega$ es un factor de regularizaci√≥n que penaliza la complejidad de los √°rboles.\n",
    "\n",
    "Ya que esta funci√≥n de p√©rdida es compleja, se utiliza su expansi√≥n de Taylos de grado 2, ya que esta es f√°cil de optimizar.\n",
    "\n",
    "$$f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2$$\n",
    "\n",
    "Aplicando el gradiente y hessiano con esta l√≥gica, la funci√≥n de p√©rdida queda de la manera:\n",
    "\n",
    "$$\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)$$\n",
    "\n",
    "Ya que el modelo utiliza √°boles de decisi√≥n de le agrega la constante $w_j$ a la funci√≥n de p√©rdida, de modo que se logre regularizar:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Posteriormente, se minimiza el t√©rmino por hoja respecto a $w_j$, obteniendo el output value.\n",
    "\n",
    "$$\\text{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}$$\n",
    "\n",
    "Una vez que se tienen estos valores se calcula la mejora que hay en cada hoja por utilizar el √≥ptimo:\n",
    "\n",
    "$$\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}$$\n",
    "\n",
    "Con base en este valor, se obtine el gain, para determinar si es √∫til realizar un split. Si el gain es positivo, se realiza el split, si es negativo se detiene.\n",
    "\n",
    "$$\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma$$\n",
    "\n",
    "Finalmente con el √°rbol terminado se realiza la predicci√≥n final, sumando todos los √°rboles\n",
    "\n",
    "$$\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "Cuando usar cada modelo:\n",
    "\n",
    "| Situaci√≥n                                                  | Recomendaci√≥n                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular peque√±o o mediano                          | ‚úÖ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables num√©ricas                 | ‚úÖ LightGBM                                         |\n",
    "| Muchas variables categ√≥ricas sin preprocesamiento          | ‚úÖ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ‚úÖ XGBoost (muy probado en producci√≥n y Kaggle)     |\n",
    "| Entrenamiento r√°pido con buen desempe√±o                    | ‚úÖ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ‚úÖ Cualquiera, pero CatBoost da mejores resultados con categ√≥ricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ‚úÖ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ‚úÖ XGBoost o LightGBM                               |\n",
    "| Tuning autom√°tico (Optuna, GridSearchCV, etc.)             | ‚úÖ LightGBM (r√°pido y convergente)                  |\n",
    "| Producci√≥n en sistemas legacy o APIs bien documentadas     | ‚úÖ XGBoost (mayor madurez, m√°s integraci√≥n)         |\n",
    "| Clasificaci√≥n multi-label o problemas no est√°ndar          | ‚úÖ XGBoost (soporte m√°s flexible)                   |\n",
    "\n",
    "---\n",
    "\n",
    "- **LightGBM** puede overfittear f√°cilmente ‚Üí cuida `num_leaves` y `min_data_in_leaf`.\n",
    "- **CatBoost** funciona muy bien con defaults y sin preprocessing.\n",
    "- **XGBoost** es muy robusto y balanceado, ideal si ya tienes un pipeline con encoding hecho.\n",
    "\n",
    "La estructura del √°rbol:\n",
    "\n",
    "* XGBoost produce √°rboles m√°s sim√©tricos y balanceados.\n",
    "\n",
    "* LightGBM produce √°rboles m√°s profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisi√≥n y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta m√°s f√°cil.\n",
    "\n",
    "* Level-wise (XGBoost) es m√°s estable, pero a veces menos preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
