{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d323cad6-0afa-4c28-9ce0-07c970efdb6b",
   "metadata": {},
   "source": [
    "# Examen – Modelos de Ensamble: Árboles y Boosting\n",
    "\n",
    "**Instrucciones**: Justifica cada respuesta de manera clara. Usa fórmulas donde aplique y sé específico en tus argumentos.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección I: Árboles de decisión (15 puntos)\n",
    "\n",
    "### 1. (5 pts)  \n",
    "Explica cómo se construye un árbol y qué criterio usa para decidir los splits. Explica tanto para el caso de clasificación como de regresión.\n",
    "\n",
    "Un árbol de decisión es un modelo de predicción, el cual utiliza distinos niveles de clasificación de datos con base en cierto valor (umbral). Divide los datos en grupos (ramas) que sobrepasan o no ese umbral. El modelo de regresión utiliza todas las varibles y splits posibles, donde finalmente aplica el tipo de split con mayor reducción de varianza, con base en el error cuadrático medio. De manera similar a la regresión, el calsificador utiliza todas las variables y umbrales posibles, eligiendo en este caso el umbral que maximice la pureza, con base en Gini o entropía.\n",
    "\n",
    "### 2. (5 pts)  \n",
    "Da un ejemplo de sobreajuste en un árbol de decisión. Explica cómo se podría evitar sin necesidad de usar ensambles.\n",
    "\n",
    "El ejemlo más básico de sobre ajuste en un árbol es si la profundidad del árbol es mucha, ya que finalmente se estaría creando una nueva rama para cada escenario posible, lo que equivale a la memorización de datos.\n",
    "\n",
    "### 3. (5 pts)  \n",
    "Si te fijas, en clase nunca hicimos escalamiento (`StandardScaler`). ¿Por qué los árboles no lo necesitan?\n",
    "\n",
    "No lo necesita, ya que no compara a todos los datos simultaneamente, sino que en cada nodo u hoja toma una variable base con cual toma la decisión de dividir los datos en ramas. Ya que en cada hoja se estan filtrando los datos únicamente por una única variable no es necesario escalar.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección II: Random Forest (20 puntos)\n",
    "\n",
    "### 4. (10 pts)  \n",
    "Explica cómo funciona un Random Forest. ¿En qué se basa? ¿Por qué es una buena idea?\n",
    "\n",
    "Un random forest está basado en Bootstrap. El algoritmo realiza este resampleo de datos con boostrap muchas veces, y con cada uno de ellos realiza un árbol de decisión. Finalmente promedia la predicción de todos los diferentes árboles y toma ese valor como predicción del modelo de RF.\n",
    "\n",
    "### 5. (10 pts)  \n",
    "Menciona dos ventajas y dos desventajas del Random Forest comparado con un solo árbol. Sé específico, no generalices.\n",
    "\n",
    "Ventajas:\n",
    "- Al hacer bootstrap se limita el overfitting que se puede llegar a crear, porque en cada árbol se trabaja con diferentes datos de un mismo dataset y el resultado es el promedio de todo.\n",
    "- Podría verse como una especie de reductor de varianza, ya que por el mismo bootsrap habría casos en los que no considere outliers, o que tome menos de estos datos en cuanta, mostrando un mejor ajuste.\n",
    "\n",
    "Desventajas:\n",
    "- \n",
    "- \n",
    "\n",
    "---\n",
    "\n",
    "## Sección III: Gradient Boosting (25 puntos)\n",
    "\n",
    "### 6. (10 pts)  \n",
    "Explica, paso a paso, cómo funciona el algoritmo de **Gradient Boosting**. Incluye el concepto de residuales y cómo se minimiza la pérdida en cada iteración.\n",
    "\n",
    "### 7. (15 pts)  \n",
    "¿Cuál es la diferencia entre Gradient Boosting y Random Forest en términos de cómo combinan los árboles? \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Sección IV: XGBoost (20 puntos)\n",
    "\n",
    "### 8. (10 pts)  \n",
    "Explica cómo XGBoost optimiza el proceso de boosting usando una expansión de Taylor de segundo orden.\n",
    "\n",
    "### 9. (5 pts)  \n",
    "¿Qué es el *similarity score*? ¿Qué es el *output value*? ¿De dónde salen estas fórmulas y cuál es su interpretación?\n",
    "\n",
    "### 10. (5 pts)  \n",
    "XGBoost y otros modelos de gradient boosting permiten evaluar la importancia de las variables con diferentes métricas: `weight` y `gain`. Explica qué representa cada una. ¿Cuál crees que es más útil para interpretar un modelo y por qué?\n",
    "\n",
    "---\n",
    "\n",
    "## Sección V: LightGBM y CatBoost (15 puntos)\n",
    "\n",
    "### 10. (5 pts)  \n",
    "Explica qué es **histogram-based splitting** y cómo lo implementa LightGBM para ganar velocidad.\n",
    "\n",
    "### 11. (5 pts)  \n",
    "¿Qué problema específico resuelve CatBoost respecto al manejo de variables categóricas? ¿Cómo lo hace?\n",
    "\n",
    "### 12. (5 pts)  \n",
    "Compara LightGBM y CatBoost: ¿cuándo usarías uno sobre el otro? Sé claro y justifica en base a tipo de datos, velocidad o precisión.\n",
    "\n",
    "## Sección VI: Power Analysis (5 puntos)\n",
    "\n",
    "### 13. (5 pts)  \n",
    "¿Qué es un *power analysis* y para qué sirve? ¿En qué contexto lo hemos usado en clase y por qué es importante antes de correr un experimento?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782e43b-cda4-43f3-998c-f3f535111a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
