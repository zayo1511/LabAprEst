{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb4a27-9f28-435e-8a52-8a9351c7d47d",
   "metadata": {},
   "source": [
    "# Examen 4\n",
    "\n",
    "## Sección 1: Interpretabilidad con SHAP (25 puntos)\n",
    "\n",
    "**1.1** (Teoría, 10 pts)  \n",
    "¿Qué representa un valor SHAP en el contexto de un modelo de machine learning? ¿Cuál es su fundamento teórico?\n",
    "\n",
    "Los Shap values ayudan a ver cual es la desviación que existe de las variables sobre la predicción. Es decir mide que tanto impacto tiene cada una de las variables en cuanto l aumento o disminución de probabilidad de predicción de x valor. En el ejemplo de diabetes indican que tanto influyen para que modelo logre predecir como 1 o como 0.\n",
    "\n",
    "Se ordenan las variables con cada una de las permutaciones posibles. Para cada una de esos ordenes se calcula la contribución marginal por feature en ese orden en específico, para despues promediarlos y obtener el valor de Shap. La contribución marginal de una variable es el cambio de la predicción del modelo al agregar esa variable a un conjunto de variables ya conocidas. Esto nace por la teoría de juegos cooperativos, en donde se busca la combinación de acciones que maximice el pago de los jugadores en conjunto, por lo que la contribución marginal media es la manera más \"justa\" de repartirlo.\n",
    "\n",
    "**1.2** (Cálculo, 10 pts)  \n",
    "Un modelo predice que un cliente tendrá una probabilidad de impago del 0.78. El valor esperado del modelo es 0.5. Los SHAP values para tres variables son:  \n",
    "- Edad: +0.10  \n",
    "- Ingreso mensual: -0.05  \n",
    "- Historial crediticio: +0.23  \n",
    "\n",
    "¿La suma es consistente con la predicción? Explica.\n",
    "\n",
    "Si, la suma si es concistente con la predicción. El modelo comienza en 0.5, la edad de la persona en específico analizada suma 0.1, por lo que nos encontramos en 0.6, sin embargo, su ingreso mensual se aleja 0.05 llegando a 0.55, pero finalmente el historial crediticio afecta en la predicción sumando 0.23, obteniendo de esa manera el 0.78 equivalente a la predicción de probabilidad de impago.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 2: K-Means Clustering (20 puntos)\n",
    "\n",
    "**2.1** (Teoría, 10 pts)  \n",
    "Explica que hace KMeans, el algoritmo, como encuentra clusters, etc.\n",
    "\n",
    "Es un algorítmo de clusterización que divide un conjunto de datos en K grupos, de modo que los elementos dentro del mismo grupo sean lo más parecidos entre si posibles. Si se quiere dividir los clusters por cercanía los pasos para dividir son:\n",
    "\n",
    "1. Inicializa K centroides (centros de grupo) de forma aleatoria.\n",
    "2. Asigna cada punto al centro más cercano (según distancia euclidiana).\n",
    "3. Recalcula los centroides como el promedio de los puntos asignados a cada grupo.\n",
    "\n",
    "Repite los pasos 2 y 3 hasta que converge.\n",
    "\n",
    "La principal limitación de este algoritmo en concreto es su manera de clusterzación vertical, ya que en muchos casos hay una clara diferencia de grupos de manera horizontal, sin embargo, el modelo no cuenta con el alcance de captar esto.\n",
    "\n",
    "**2.2** (Criterio, 10 pts)  \n",
    "Explica cómo usarías el método del codo (*elbow method*) y qué limitaciones tiene.\n",
    "\n",
    "El método del codo te ayuda a ver la disminución de la suma de errores al cuadrado por cada cluster nuevo que se agrega. Normalmente mientras más cluesters menor es el error, sin embargo, se selecciona el númeor de clusters que se encuentra en el \"codo\" de la gráfica. El codo representa el punto donde es considerable la disminución de error, teniendo en cuenta el costo de agregar un cluster más. Las limitaciones que tiene este método es que es no existe una regla 100% establecida, sino que viendo la gráfica el analista debe de decir cual es el óptimo lo que lo hace totalmente subjetivo y puede generar distintos resultados para un mismo dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 3: PCA – Análisis de Componentes Principales (20 puntos)\n",
    "\n",
    "**3.1** (Teoría, 5 pts)  \n",
    "¿Qué significa que la primera componente principal maximiza la varianza?\n",
    "\n",
    "El PCA busca maximizar la varianza, ya que busca crear nuevas variables, a forma de combinación lineal que logren explicar la mayor cantidad de información del dataset posible. Ya que se busca reducir la dimensionalidad, se busca explicar la mayor cantidad de variable posible, con la menor cantidad de variables posible. Siempre, la PC1 es la que más explica ya que es la primera en ser calculada, es decir la \"mejor\", de ahí se van creando diferentes variables que aunque igual expliquen mucho no logran explicar tanto como la primera.\n",
    "\n",
    "**3.2** (Cálculo, 10 pts)  \n",
    "Si tienes 10 variables correlacionadas y aplicas PCA, ¿cuántas componentes necesitas para explicar al menos el 90% de la varianza? Describe cómo se respondería esta pregunta\n",
    "\n",
    "Aplicando PCA sobre las 10 variables se obtendrían 10 componentes. Cada uno tiene una explicación de varianza de manera individual, para llegar al 90% se debería de sumar la contribución individual de las primeras n componentes hasta llegar al 90%. Supongamos que de las 10 componentes se sabe que la explicación de varianza de las primeras 5 es 0.31, 0.29, 0.18, 0.12 y 0.07. Únicamente se deberían de utilizar las primeras 4, ya que en conjunto de esas 4 se explica el 90% de la varianza de los datos $(0.31+0.29+0.18+0.12=0.9=90\\%)$, ya no sería necesario sumar el 0.07 de la variable numero 5 ya que se estaría sobredimensionando el dataset por una explicación baja de varianza adiocional.\n",
    "\n",
    "**3.3** (Aplicación, 5 pts)  \n",
    "¿Tiene sentido usar PCA y usar todas las componentes para predecir? Justifica tu respuesta.\n",
    "\n",
    "Tiene sentido numérico, ya que el resultado obtenido va a ser bastante bueno, sin embargo, como el principal uso del PCA es la reducción de dimensionalidad, no tendría sentido practico aplicarlo de esta manera. Más bien se buscaria ultilar el n número de PC que llegue a x nivel de varianza explicada (90% por ejemplo). Ya que las ultimas componentes explican muy poca varianza, se puede disminuir considerablemnte el número de variables al no incluirlas, obteniendo igualmente una alta explicación de varianza.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 4: Causalidad y Meta-Learners (35 puntos)\n",
    "\n",
    "**4.1** (Conceptual, 10 pts)  \n",
    "Define el estimando CATE y da un ejemplo de aplicación en negocios.\n",
    "\n",
    "El CATE es un valor que mide el impacto de una tratamiento, condicionado a un conjunto de características. Una aplicación en negocios, sería el aplicar descuentos en alguna tienda. Se tienen los datos de consumo sin y con descuento, por lo que el descuento es el tratamiento. Se compara el patron de consumo de los clientes para determianr si los descuentos aplicados verdaderamente tienen un impacto y de ese modo determinar si aplicar o no descuentos más adelante.\n",
    "\n",
    "**4.2** (Comparación, 15 pts)  \n",
    "Explica las diferencias entre los siguientes enfoques para estimar CATE:\n",
    "- S-Learner  \n",
    "- T-Learner  \n",
    "- X-Learner  \n",
    "\n",
    "Incluye ventajas y desventajas.\n",
    "\n",
    "El **S-Learner** usa solo un modelo predictivo donde se incluye el tratamiento como una característica adicional para predecir. Se entrena con todos los datos (tratados y no tratados) y estima el efecto del tratamiento prediciendo el resultado con y sin tratamiento para cada individuo. Su ventaja es ser simple y útil cuando el tratamiento no es muy influyente, pero su desventaja es que puede subestimar el efecto causal ya que es un único modelo. para ambar predicciones.\n",
    "\n",
    "El **T-Learner** entrena dos modelos separados: uno para el grupo con tratamiento y otro para el grupo sin trtamiento. Luego, el CATE se estima como la diferencia entre las predicciones de ambos modelos. Esto permite mayor flexibilidad, cuando el tratamiento tiene efectos muy distintos en cada grupo. Sin embargo, puede tener problemas si uno de los grupos es mucho más pequeño, ya que el modelo correspondiente puede estar mal ajustado por una diferencia de fit (insuficiencia de datos). \n",
    "\n",
    "El **X-Learner**, por su parte, busca mejorar esto: primero entrena como el T-Learner, pero después calcula efectos individuales de tratamiento y ajusta las predicciones ponderando según su \"atracción\" al tratamiento. El X-Learner es bastante útil en escenarios con diferencia de datos por tratamiento, pero también es más complejo y costoso. Cuando funciona es el mejor, sin embargo, no siempre funciona.\n",
    "\n",
    "**4.3** (Implementación, 10 pts)  \n",
    "Te entregan un dataset con una columna `treatment` (0/1), un `outcome`, y varias covariables. ¿Cómo entrenarías un X-Learner paso a paso?\n",
    "\n",
    "- Se hacen 2 modelos como en el T learner. \n",
    "- Con estos modelos se calculan D0 (Lo que pasó - lo que pasaría sin tratamiento) y D1 (Lo que pasaría con el tratamiento - lo que pasó). \n",
    "- Ya que se tienen calculados D0 y D1, entenas dos nuevos modelos que se usarán para predecir el CATE condicionad dentro de cada grupo (Se calcula el CATE sin tratamiento y CATE con tratamiento). Esto se hace con los modelos mD0 (tratamiento como X y D0 como y) y mD1 (tratamiento como X y D1 como y). \n",
    "- Finalmente se calcula el propensity score que mide la probabilidad de tener tratamiento condicional a X. Esto se calcula con un nuevo modelo en el que los datos originales son X y tener tratamiento o no es y, realizando la predicción de probabilidades. \n",
    "  - Con estas probabilidades se pondera el CATE para darle el peso adecuado a tener o no tratamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c0b49-9d6b-406a-996b-6f61c008280b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
