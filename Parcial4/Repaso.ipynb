{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# KMeans\n",
    "\n",
    "Es un algorítmo de clusterización que divide un conjunto de datos en K grupos, de modo que los elementos dentro del mismo grupo sean lo más parecidos entre si posibles. Si se quiere dividir los clusters por cercanía los pasos para dividir son:\n",
    "\n",
    "1. Inicializa K centroides (centros de grupo) de forma aleatoria.\n",
    "2. Asigna cada punto al centro más cercano (según distancia euclidiana).\n",
    "3. Recalcula los centroides como el promedio de los puntos asignados a cada grupo.\n",
    "\n",
    "Repite los pasos 2 y 3 hasta que converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap Values\n",
    "\n",
    "Se ordenan las variables con cada una de las perfmutaciones posibles. Para cada una de esos ordenes se calcula la contribución marginal por feature en ese orden en específico, para despues promediarlos y obtener el valor de Shap.\n",
    "\n",
    "La contribución marginal de una variable es el cambio de la predicción del modelo al agregar esa variable a un conjunto de variables ya conocidas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Técnica que busca disminuir la dimensionalidad de un conjunto de datos, manteniendo la mayor cantidad de varianza posible. Para esto se necesita la proyección del componente:\n",
    "\n",
    "$$\\text{Proy}_{u_1}(x_i) = u_1^T x_i$$\n",
    "\n",
    "El objetivo es tener la cantidad de componentes que expliquen la mayor cantidad de varianza posible:\n",
    "\n",
    "$$\\text{Varianza} = \\frac{1}{N} \\sum_{n=1}^N \\left( u_1^T x_n - u_1^T \\bar{x} \\right)^2$$\n",
    "\n",
    "Finalmente, agrupando términos:\n",
    "\n",
    "$$= u_1^T S u_1$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $S$ es la matriz de covarianza de los datos.\n",
    "- $u_1$ indica la dirección del primer componente principal, es decir, la dirección donde los datos se extienden más.\n",
    "\n",
    "Se busca encontrar el vector $u_1$ que maximice la varianza de las proyecciones.\n",
    "\n",
    "$$\\max_{u_1} u_1^T S u_1$$\n",
    "\n",
    "sujeto a la restricción:\n",
    "\n",
    "$$\\|u_1\\| = 1$$\n",
    "\n",
    "Aplicando multiplicadores de Lagrange se tiene:\n",
    "\n",
    "$$\\mathcal{L}(u_1, \\lambda) = u_1^T S u_1 - \\lambda (u_1^T u_1 - 1)$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial u_1} = 2 S u_1 - 2 \\lambda u_1 = 0$$\n",
    "\n",
    "lo que se puede reorganizar como:\n",
    "\n",
    "$$S u_1 = \\lambda u_1$$\n",
    "\n",
    "La expresión a la que se llega es un problema de eigenvalores que maximiza la varianza asociada al componente principal, donde:\n",
    "\n",
    "- $u_1$ es un eigenvector\n",
    "- $\\lambda$ es el eigenvalor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S learner\n",
    "\n",
    "## CATE\n",
    "\n",
    "Efecto esperado del tratamiento para los individuos con dicha característica.\n",
    "\n",
    "## Tratamiento continuo\n",
    "\n",
    "Se entrena el modelo sobre los datos originales, para posteriormente, sobre una observación, analizar su efecto. A esta variable se le dan múltiples valores distintos y de manera iterativa se utiliza el modelo para predecir y ver que efecto hay sobre la predicción con los distintos valores. \n",
    "\n",
    "## Tratamiento discreto\n",
    "\n",
    "Se entrena el modelo sobre los datos originales, para posteriormente modificar los datos para que todos tengan el tratamiento 1 y otro donde todos tengan 0. Estos nuevos datos se utilizan para hacer predicciones. A las predicciones con tratamiento se les restan las predicciones sin tratamiento, donde la diferencia es el CATE de tener tratamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T learner\n",
    "\n",
    "Se modifican los datos originales para tener dos datasets uno con y uno sin tratamiento. Con estos datos se entrenan dos modelos diferentes uno con y otro sin tratamiento. Con estos dos modelos diferentes se predice sobre los datos de test originales. Ya que uno de los modelos tiene un sesgo de tratamiento y otro tiene sesgo sin tratamiento se pueden restar las predicciones de ambos modelos para encontrar el CATE del tratamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X learner\n",
    "\n",
    "Se hacen 2 modelos como en el T learner. Con estos modelos se calculan D0 (Lo que pasó - lo que pasaría sin tratamiento) y D1 (Lo que pasaría con el tratamiento - lo que pasó). Ya que se tienen calculados D0 y D1, entenas dos nuevos modelos que se usarán para predecir el CATE condicionad dentro de cada grupo (Se calcula el CATE sin tratamiento y CATE con tratamiento). Esto se hace con los modelos mD0 (tratamiento como X y D0 como y) y mD1 (tratamiento como X y D1 como y). Finalmente se calcula el propensity score que mide la probabilidad de tener tratamiento condicional a X. Esto se calcula con un nuevo modelo en el que los datos originales son X y tener tratamiento o no es y, realizando la predicción de probabilidades. Con estas probabilidades se pondera el CATE para darle el peso adecuado a tener o no tratamiento."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
