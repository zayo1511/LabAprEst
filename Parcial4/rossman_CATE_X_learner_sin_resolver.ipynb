{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aff1812-3c90-492c-91a5-fc4bdd9e127c",
   "metadata": {},
   "source": [
    "# Rossman dataset \n",
    "\n",
    "Fuente lightgbm: https://lightgbm.readthedocs.io/en/stable/\n",
    "\n",
    "https://www.kaggle.com/competitions/rossmann-store-sales/data\n",
    "\n",
    "- Id - an Id that represents a (Store, Date) duple within the test set\n",
    "- Store - a unique Id for each store\n",
    "- Sales - the turnover for any given day (this is what you are predicting)\n",
    "- Customers - the number of customers on a given day\n",
    "- Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "- StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "- SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "- Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
    "- CompetitionDistance - distance in meters to the nearest competitor store\n",
    "- CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
    "- Promo - indicates whether a store is running a promo on that day\n",
    "- Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
    "- Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
    "- PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e87506d-9256-411c-a66b-569f18f3a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/z6bgw1fj4vng6rs97g1chhz00000gn/T/ipykernel_4555/248155395.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('rossman.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation, LGBMClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# CARGA Y PREPROCESAMIENTO\n",
    "# ----------------------------\n",
    "# Carga de datos\n",
    "train = pd.read_csv('rossman.csv')\n",
    "stores = pd.read_csv('store.csv')\n",
    "\n",
    "# Merge de ambos datasets\n",
    "df = pd.merge(train, stores, on='Store')\n",
    "\n",
    "# Convertir la columna de fecha\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Ordenar por fecha para evitar leakage\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Filtrar solo tiendas abiertas\n",
    "df = df[df['Open'] == 1]\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# ----------------------------\n",
    "\n",
    "# Variables temporales útiles\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# Eliminar columnas que no usaremos\n",
    "df = df.drop(columns=['Date', 'Store', 'Customers', 'Open', 'Promo2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e20e58-f596-4cfb-a062-6aa2a781579a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3425b0f8-8557-4015-b5ae-4ba162554a7d",
   "metadata": {},
   "source": [
    "### ⚙️ Tuneando LightGBM\n",
    "\n",
    "- **`num_iterations`** (`n_estimators` en `sklearn`):  \n",
    "  Número total de árboles a entrenar. Si usas `early_stopping_rounds`, puedes poner un número alto sin preocuparte por overfitting.\n",
    "\n",
    "- **`max_depth`**:  \n",
    "  Profundidad máxima de cada árbol. Profundidades mayores capturan más complejidad, pero pueden sobreajustar.\n",
    "\n",
    "- **`learning_rate`**:  \n",
    "  Qué tan rápido aprende el modelo. Valores más bajos requieren más árboles, pero suelen generalizar mejor.\n",
    "\n",
    "- **`subsample`** (`bagging_fraction`):  \n",
    "  Fracción de observaciones usadas en cada árbol. Ayuda a reducir overfitting. Requiere activar también `bagging_freq`.\n",
    "\n",
    "- **`feature_fraction`** (equivalente a `colsample_bytree` en XGBoost):  \n",
    "  Fracción de columnas (features) usadas en cada árbol. También ayuda a evitar sobreajuste.\n",
    "\n",
    "- **`early_stopping_rounds`**:  \n",
    "  Detiene el entrenamiento si la métrica en el conjunto de validación no mejora en N iteraciones. Requiere `valid_sets` y `eval_metric`.\n",
    "\n",
    "- **`metric`** (`eval_metric` en sklearn):  \n",
    "  Métrica usada durante entrenamiento para monitorear desempeño (ej: `'rmse'`, `'mae'`, `'binary_logloss'`, `'auc'`).\n",
    "\n",
    "- **`min_data_in_leaf`**:  \n",
    "  Número mínimo de muestras requeridas en una hoja. Sirve para evitar sobreajuste. Similar a `min_child_samples` en XGBoost y `min_data_in_leaf` en CatBoost.\n",
    "\n",
    "- **`lambda_l2`** (`reg_lambda` en sklearn):  \n",
    "  Regularización L2 (Ridge) aplicada a los pesos de las hojas. Ayuda a controlar la complejidad.\n",
    "\n",
    "- **`min_gain_to_split`** (`gamma` en XGBoost):  \n",
    "  Ganancia mínima necesaria para hacer un split. Controla qué tan fácilmente el árbol se ramifica → útil contra overfitting.\n",
    "\n",
    "\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    callbacks=[\n",
    "        early_stopping(20),\n",
    "        log_evaluation(100)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef25db0-8868-4b3d-bce9-55ca4c1676c6",
   "metadata": {},
   "source": [
    "## Conditional average treatment effect (CATE)\n",
    "\n",
    "Para una covariable \\( X = x \\), el CATE se define como:\n",
    "\n",
    "$$\n",
    "\\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]\n",
    "$$\n",
    "\n",
    "> Es decir, **el efecto esperado del tratamiento para individuos con características \\( X = x \\)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4ca11-c1fe-47f5-ab84-dc169f06ed52",
   "metadata": {},
   "source": [
    "# X-Learner: Estimación paso a paso\n",
    "\n",
    "El **X-Learner** es un meta-learner diseñado para estimar efectos causales heterogéneos (CATEs) cuando el tratamiento es binario y el dataset está desbalanceado. A continuación, se presenta el procedimiento detallado:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Entrenar modelos por grupo (como un T-Learner)\n",
    "\n",
    "Entrena dos modelos para predecir el resultado observado en cada grupo:\n",
    "\n",
    "- $M_1(x) \\approx \\mathbb{E}[Y \\mid X = x, T = 1]$  \n",
    "- $M_0(x) \\approx \\mathbb{E}[Y \\mid X = x, T = 0]$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Imputar efectos individuales contrafactuales\n",
    "\n",
    "Aquí calculamos la diferencia entre el resultado observado y el resultado contrafactual estimado:\n",
    "\n",
    "- Para individuos **tratados** ($T = 1$):  \n",
    "  $$\n",
    "  D^0_i = Y_i - M_0(X_i)\n",
    "  $$\n",
    "\n",
    "- Para individuos **no tratados** ($T = 0$):  \n",
    "  $$\n",
    "  D^1_i = M_1(X_i) - Y_i\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Entrenar modelos para predecir efectos (CATE)\n",
    "\n",
    "Entrena dos modelos adicionales:\n",
    "\n",
    "- $\\hat{\\tau}_0(x)$ usando $\\{X_i, D^0_i\\}$ para individuos tratados  \n",
    "- $\\hat{\\tau}_1(x)$ usando $\\{X_i, D^1_i\\}$ para individuos no tratados\n",
    "\n",
    "Estos modelos aprenden a predecir el efecto causal condicional dentro de cada grupo.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Estimar el propensity score\n",
    "\n",
    "Calcula la probabilidad de recibir tratamiento condicional a $X$:\n",
    "\n",
    "$$\n",
    "\\hat{e}(x) = \\mathbb{P}(T = 1 \\mid X = x)\n",
    "$$\n",
    "\n",
    "Este propensity score puede ser estimado con regresión logística, boosting, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Calcular el efecto causal final con ponderación\n",
    "\n",
    "El estimador final combina las dos predicciones de CATE usando el propensity score:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}(x) = (1 - \\hat{e}(x)) \\cdot \\hat{\\tau}_0(x) + \\hat{e}(x) \\cdot \\hat{\\tau}_1(x)\n",
    "$$\n",
    "\n",
    "> Así se da más peso a la predicción que proviene del grupo opuesto al observado.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Resumen\n",
    "\n",
    "| Paso | Acción |\n",
    "|------|--------|\n",
    "| 1 | Estimar $M_0(x)$ y $M_1(x)$ como en T-Learner |\n",
    "| 2 | Imputar efectos individuales con contrafactuales |\n",
    "| 3 | Entrenar modelos $\\hat{\\tau}_0(x)$ y $\\hat{\\tau}_1(x)$ |\n",
    "| 4 | Estimar $\\hat{e}(x)$, el propensity score |\n",
    "| 5 | Calcular $\\hat{\\tau}(x)$ con una combinación ponderada |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55cf15-779d-4196-b15d-52cfb827a036",
   "metadata": {},
   "source": [
    "## Paso 1: T-learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "331d341f-9c6d-41ef-9453-333898e36968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# SEPARACIÓN TEMPORAL: 80% pasado, 20% futuro\n",
    "# ----------------------------\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "\n",
    "# Separar X y y\n",
    "target = 'Sales'\n",
    "X_train = train_df.drop(columns=target)\n",
    "y_train = train_df[target]\n",
    "X_test = test_df.drop(columns=target)\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Guardar columna Promo antes de get_dummies\n",
    "promo_train = X_train['Promo']\n",
    "promo_test = X_test['Promo']\n",
    "\n",
    "# Codificar variables categóricas\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Alinear columnas\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_train.columns = X_train.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "X_test.columns = X_test.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "\n",
    "# ----------------------------\n",
    "# SEPARAR POR PROMO (entrenamiento y prueba)\n",
    "# ----------------------------\n",
    "\n",
    "# Detectar la columna codificada de Promo\n",
    "promo_col = [col for col in X_train.columns if 'Promo' in col][0]  # asume binaria\n",
    "\n",
    "# Train: Promo = 1\n",
    "X_train_promo1 = X_train[X_train[promo_col] == 1]\n",
    "y_train_promo1 = y_train.loc[X_train_promo1.index]\n",
    "\n",
    "# Train: Promo = 0\n",
    "X_train_promo0 = X_train[X_train[promo_col] == 0]\n",
    "y_train_promo0 = y_train.loc[X_train_promo0.index]\n",
    "\n",
    "# Test: Promo = 1\n",
    "X_test_promo1 = X_test[X_test[promo_col] == 1]\n",
    "y_test_promo1 = y_test.loc[X_test_promo1.index]\n",
    "\n",
    "# Test: Promo = 0\n",
    "X_test_promo0 = X_test[X_test[promo_col] == 0]\n",
    "y_test_promo0 = y_test.loc[X_test_promo0.index]\n",
    "\n",
    "# ----------------------------\n",
    "# CREAR VECTORES BINARIOS DE TRATAMIENTO\n",
    "# ----------------------------\n",
    "y_train_promo = promo_train.reset_index(drop=True)\n",
    "y_test_promo = promo_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58a9aa2-7095-400a-9ba8-65814d3c4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo sin promo\n",
    "\n",
    "# Entrenar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bb144d-3b6e-4772-9d16-061e2e713a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo con promo\n",
    "\n",
    "# Entrenar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e666f4-acd2-41e6-8ac3-2ac260f8d11c",
   "metadata": {},
   "source": [
    "# Parte 2\n",
    "\n",
    "Entonces $D^0_i$ es una estimación del efecto causal individual para ese individuo, inferido desde el contrafactual no tratado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b676820-f0ed-43ce-8d37-f28da92d6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar efectos individuales\n",
    "# Para los tratados: Y observado menos predicción contrafactual (como si no hubieran sido tratados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39acaa0-0dd5-4bde-8dcb-52dde2366630",
   "metadata": {},
   "source": [
    "Entonces $D^1_i$ también es una estimación del efecto causal individual, pero desde el lado opuesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30773666-2364-449d-9336-c7c9d4e92658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo que habría pasado con tratamiento menos lo que pasó\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd3cf8-1024-4370-9e23-3131c0e28ab1",
   "metadata": {},
   "source": [
    "### Entrena los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e2c213-060e-4b69-aea2-c2ea6b4dc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los modelos de efecto (CATEs)\n",
    "# Usa tratados y sus efectos imputados\n",
    "\n",
    "# Usa no tratados y sus efectos imputados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a631c-de17-4dd8-a836-4715bc366e65",
   "metadata": {},
   "source": [
    "# Parte 4 : Propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85a6031-564e-4422-8a27-fe465e7c474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columna dummy de Promo para entrenar el modelo\n",
    "\n",
    "# Entrenar modelo de Propensity Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5019d-8a09-4746-9b3f-630656f29479",
   "metadata": {},
   "source": [
    "# Parte 5: Predecir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad8ba0d9-9264-48e7-a8d7-26d8338c38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predice con tu X test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999c07b-4234-4de9-bef0-fad9bf44de2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405010-8c06-44f8-af4d-e74a25c194ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef41f7-b20f-4571-82ad-90f47b447315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
