{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regresión Softmax: Explicación Teórica**\n",
    "\n",
    "La **regresión Softmax** es una generalización de la regresión logística para problemas de clasificación multiclase. Mientras que la regresión logística se usa para clasificar entre dos clases (0 y 1), la regresión Softmax permite clasificar entre múltiples clases modelando probabilidades para cada una de ellas.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Origen: Generalización de la Regresión Logística**\n",
    "\n",
    "En regresión logística binaria, modelamos la probabilidad de que una instancia $\\mathbf{x}$ pertenezca a la clase $y = 1$ usando la función sigmoide:\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\mathbf{x}) = \\frac{1}{1 + \\exp(- (\\mathbf{w} \\cdot \\mathbf{x} + b))}\n",
    "$$\n",
    "\n",
    "Para clasificación multiclase, necesitamos una función que asigne probabilidades a **múltiples clases** en lugar de solo una. Esto se logra con la **función Softmax**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Definición de la Función Softmax**\n",
    "\n",
    "La función Softmax convierte los logits (salidas de la función lineal) en probabilidades:\n",
    "\n",
    "$$\n",
    "P(y = k | \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k \\cdot \\mathbf{x} + b_k)}{\\sum_{j=1}^{K} \\exp(\\mathbf{w}_j \\cdot \\mathbf{x} + b_j)}\n",
    "$$\n",
    "\n",
    "### **¿Qué significa esta ecuación?**\n",
    "- Para cada clase $k$, calculamos un **logit** con su propio vector de pesos $\\mathbf{w}_k$ y bias $b_k$.\n",
    "- Aplicamos la función exponencial para mantener las probabilidades **siempre positivas**.\n",
    "- Normalizamos dividiendo entre la suma de todas las exponenciales, asegurando que las probabilidades sumen 1.\n",
    "\n",
    "#### **Ejemplo Numérico**\n",
    "Si tenemos 3 clases y logits calculados como:\n",
    "$$\n",
    "z_1 = 2.0, \\quad z_2 = 1.0, \\quad z_3 = -1.0\n",
    "$$\n",
    "Aplicamos Softmax:\n",
    "$$\n",
    "P(y=1) = \\frac{e^2}{e^2 + e^1 + e^{-1}} = 0.72\n",
    "$$\n",
    "$$\n",
    "P(y=2) = \\frac{e^1}{e^2 + e^1 + e^{-1}} = 0.26\n",
    "$$\n",
    "$$\n",
    "P(y=3) = \\frac{e^{-1}}{e^2 + e^1 + e^{-1}} = 0.04\n",
    "$$\n",
    "Esto indica que la clase 1 es la más probable.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Función de Pérdida: Entropía Cruzada**\n",
    "\n",
    "Para entrenar el modelo, usamos la **entropía cruzada**:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} \\mathbb{1}(y_i = k) \\log P(y_i = k | \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "### **¿Qué significa esta ecuación?**\n",
    "- Es una suma sobre todas las muestras ($N$) y todas las clases ($K$).\n",
    "- $\\mathbb{1}(y_i = k)$ es 1 si la clase correcta es $k$, y 0 en caso contrario.\n",
    "- Penaliza predicciones erróneas con mayor intensidad.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Gradiente Descendente: Cómo se Ajustan los Pesos**\n",
    "\n",
    "Para minimizar la pérdida, calculamos el gradiente de $L$ respecto a los pesos de la clase $k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_k} = \\sum_{i=1}^{N} \\left(P(y_i = k | \\mathbf{x}_i) - \\mathbb{1}(y_i = k)\\right) \\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "**Actualización de pesos con descenso en gradiente:**\n",
    "$$\n",
    "\\mathbf{w}_k \\leftarrow \\mathbf{w}_k - \\eta \\frac{\\partial L}{\\partial \\mathbf{w}_k}\n",
    "$$\n",
    "\n",
    "Donde $\\eta$ es la **tasa de aprendizaje**.\n",
    "\n",
    "### **¿Cómo interpretamos el gradiente?**\n",
    "- Si la predicción $P(y_i = k)$ es **mayor** que la etiqueta real, reducimos $\\mathbf{w}_k$.\n",
    "- Si es **menor**, aumentamos $\\mathbf{w}_k$.\n",
    "\n",
    "Esto ajusta el modelo para mejorar la clasificación en cada iteración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# One-hot encoding de las etiquetas\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m y_one_hot \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Dividir en entrenamiento y prueba\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "\n",
    "# Cargar el dataset Wine\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Crear un DataFrame con nombres de columnas\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df[\"target\"] = y  # Agregar la columna de la clase\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "# One-hot encoding de las etiquetas\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicialización de pesos y bias\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(X.shape[1], y_one_hot.shape[1]) * 0.01\n",
    "b = np.zeros((1, y_one_hot.shape[1]))\n",
    "\n",
    "# Función Softmax\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Parámetros del entrenamiento\n",
    "lr = 0.1\n",
    "num_epochs = 500\n",
    "loss_history = []\n",
    "\n",
    "# Gradiente Descendente\n",
    "for epoch in range(num_epochs):\n",
    "    logits = np.dot(X_train, W) + b\n",
    "    y_pred = softmax(logits)\n",
    "    \n",
    "    dW = np.dot(X_train.T, (y_pred - y_train)) / len(X_train)\n",
    "    db = np.sum(y_pred - y_train, axis=0, keepdims=True) / len(X_train)\n",
    "    \n",
    "    W -= lr * dW\n",
    "    b -= lr * db\n",
    "    \n",
    "    loss = -np.sum(y_train * np.log(y_pred + 1e-9)) / len(X_train)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Época {epoch}, Pérdida: {loss:.4f}\")\n",
    "\n",
    "# Gráfica de la función de pérdida\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.title(\"Evolución de la pérdida en Wine Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora con sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "\n",
    "# Cargar el dataset Wine\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Crear un DataFrame con nombres de columnas\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df[\"target\"] = y  # Agregar la columna de la clase\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "X = df.drop(target, axis=1)\n",
    "\n",
    "# Normalizar las características\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "# Crear y entrenar el modelo de Regresión Logística Multiclase (Softmax)\n",
    "model = linear_model.LogisticRegression(multi_class='multinomial', max_iter=500).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9325842696629213"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=predicciones, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de coeficientes\n",
    "coeficientes = pd.DataFrame({\n",
    "     'Variables': X_train.keys(),\n",
    "     'thetas0': model.coef_[0],\n",
    "     'thetas1': model.coef_[1],\n",
    "     'thetas2': model.coef_[2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variables</th>\n",
       "      <th>thetas0</th>\n",
       "      <th>thetas1</th>\n",
       "      <th>thetas2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>-0.573262</td>\n",
       "      <td>0.698250</td>\n",
       "      <td>-0.124988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>malic_acid</td>\n",
       "      <td>0.098674</td>\n",
       "      <td>-0.730715</td>\n",
       "      <td>0.632041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ash</td>\n",
       "      <td>0.358075</td>\n",
       "      <td>-0.411183</td>\n",
       "      <td>0.053108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alcalinity_of_ash</td>\n",
       "      <td>-0.213740</td>\n",
       "      <td>-0.024671</td>\n",
       "      <td>0.238411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magnesium</td>\n",
       "      <td>0.013398</td>\n",
       "      <td>0.029201</td>\n",
       "      <td>-0.042599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total_phenols</td>\n",
       "      <td>0.401390</td>\n",
       "      <td>-0.048565</td>\n",
       "      <td>-0.352825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flavanoids</td>\n",
       "      <td>0.667843</td>\n",
       "      <td>0.094847</td>\n",
       "      <td>-0.762690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nonflavanoid_phenols</td>\n",
       "      <td>-0.009667</td>\n",
       "      <td>-0.005912</td>\n",
       "      <td>0.015579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>proanthocyanins</td>\n",
       "      <td>-0.271895</td>\n",
       "      <td>0.572582</td>\n",
       "      <td>-0.300687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>color_intensity</td>\n",
       "      <td>0.162934</td>\n",
       "      <td>-0.924969</td>\n",
       "      <td>0.762036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hue</td>\n",
       "      <td>-0.008595</td>\n",
       "      <td>0.205646</td>\n",
       "      <td>-0.197052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>od280/od315_of_diluted_wines</td>\n",
       "      <td>0.288691</td>\n",
       "      <td>0.197006</td>\n",
       "      <td>-0.485697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proline</td>\n",
       "      <td>0.008538</td>\n",
       "      <td>-0.008158</td>\n",
       "      <td>-0.000380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Variables   thetas0   thetas1   thetas2\n",
       "0                        alcohol -0.573262  0.698250 -0.124988\n",
       "1                     malic_acid  0.098674 -0.730715  0.632041\n",
       "2                            ash  0.358075 -0.411183  0.053108\n",
       "3              alcalinity_of_ash -0.213740 -0.024671  0.238411\n",
       "4                      magnesium  0.013398  0.029201 -0.042599\n",
       "5                  total_phenols  0.401390 -0.048565 -0.352825\n",
       "6                     flavanoids  0.667843  0.094847 -0.762690\n",
       "7           nonflavanoid_phenols -0.009667 -0.005912  0.015579\n",
       "8                proanthocyanins -0.271895  0.572582 -0.300687\n",
       "9                color_intensity  0.162934 -0.924969  0.762036\n",
       "10                           hue -0.008595  0.205646 -0.197052\n",
       "11  od280/od315_of_diluted_wines  0.288691  0.197006 -0.485697\n",
       "12                       proline  0.008538 -0.008158 -0.000380"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z0 = (model.coef_[0] * X).sum(axis=1) + model.intercept_[0]\n",
    "z1 = (model.coef_[1] * X).sum(axis=1) + model.intercept_[1]\n",
    "z2 = (model.coef_[2] * X).sum(axis=1) + model.intercept_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927687264810914"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z0[0]) / (np.exp(z0[0]) + np.exp(z1[0]) + np.exp(z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.841016169791275"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z1[0]) / np.exp(z0[0]) + np.exp(z1[0]) + np.exp(z2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8339176204466136"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(z2[0]) / np.exp(z0[0]) + np.exp(z1[0]) + np.exp(z2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
