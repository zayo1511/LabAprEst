{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qué es?\n",
    "\n",
    "La regresión logística es un modelo de clasificación binaria en el que se calcula la probabilidad de ocurrencia. Con base en la probabilidad calculada el modelo discrimina la información de modo que la etiqueta como 1 o 0. Si $p < 0.5$ se clasifica como 0 y si $p > 0.5$ se clasifica como 1.\n",
    "\n",
    "## Cómo se calcula?\n",
    "\n",
    "Para calcular la probabilidad se utiliza el concepto de log odds, con el que se calcula el valor de z, el cual finalmente funciona para el cálcula de las probabilidades. \n",
    "\n",
    "## Verosimilitud General\n",
    "\n",
    "La función de verosimilitud es:\n",
    "\n",
    "$$L(\\theta, \\sigma^2) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Se aplica logaritmo\n",
    "\n",
    "$$\\log L(\\theta, \\sigma^2) = \\sum_{i=1}^{m} \\left( -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Para maximizar la verosilimitud se ignoran los términos constantes\n",
    "\n",
    "$$\\max_{\\theta} \\sum_{i=1}^{m} -\\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "Esto equivale a minimizar el error cuadrático:\n",
    "\n",
    "$$\\min_{\\theta} \\sum_{i=1}^{m} (y_i - \\theta^T X_i)^2$$\n",
    "\n",
    "\n",
    "Minimizar el error cuadrático con OLS (mínimos cuadrados) es lo mismo que maximizar la verosimilitud.\n",
    "\n",
    "---\n",
    "\n",
    "## Verosimilitud en regresión lineal\n",
    "\n",
    "Ya que sigue una distribución de bernoulli, la forma es:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{m} p(y_i | X_i; \\theta) = \\prod_{i=1}^{m} \\left[ \\sigma(\\theta^T X_i) \\right]^{y_i} \\cdot \\left[ 1 - \\sigma(\\theta^T X_i) \\right]^{(1 - y_i)}$$\n",
    "\n",
    "Al tomar el logaritmo de la verosimilitud**:\n",
    "\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]$$\n",
    "\n",
    "### Función de Pérdida\n",
    "Para estimar $\\theta$, **maximizamos la log-verosimilitud**. \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Odds y log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las odds son la razón de éxito frente al fracaso en donde se calculan con la fórmmula\n",
    "\n",
    "$$\\text{odds} = \\frac{p}{1-p}$$\n",
    "\n",
    "Ya que este crecimiento es exponencial, se aplica logaritmo, de modo que se linealize este comportamiento.\n",
    "\n",
    "$$log \\text{ odds} = log (\\frac{p}{1-p})$$\n",
    "\n",
    "---\n",
    "\n",
    "Una vez que se tienen los coeficientes de la regresión logística, con ayuda del simoide se calcula la combinación lineal de los coeficientes con las características (X), obteniendo el valor de z \n",
    "\n",
    "$$z=\\Theta ^T X$$\n",
    "\n",
    "Con esto se ultiliza la fórmula\n",
    "\n",
    "$$p= \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "donde p es la probabilidad de que l modelo clasifique como 1. Si la probabilidad es menor a 0.5 se clasifica como 0 y si es mayor a 0.5 de clasiifica como 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El softmax se utiliza de manera similar a la regresión logísica, sin embargo, este clasifica en más de dos clases (0 y 1) modelando las probabilidad para cada una de las clases.\n",
    "\n",
    "La función Softmax convierte las salidas de la función lineal en probabilidades:\n",
    "\n",
    "$$P(y = k | \\mathbf{x}) = \\frac{e^{\\mathbf{w}_k \\cdot \\mathbf{x} + b_k}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j \\cdot \\mathbf{x} + b_j}}$$\n",
    "\n",
    "**Ejemplo Numérico**\n",
    "\n",
    "Si tenemos 3 clases y logits calculados como:\n",
    "\n",
    "$$z_1 = 2, \\quad z_2 = 1, \\quad z_3 = -1$$\n",
    "\n",
    "Aplicamos Softmax:\n",
    "\n",
    "$$P(y=1) = \\frac{e^2}{e^2 + e^1 + e^{-1}} = 0.72$$\n",
    "\n",
    "$$P(y=2) = \\frac{e^1}{e^2 + e^1 + e^{-1}} = 0.26$$\n",
    "\n",
    "$$P(y=3) = \\frac{e^{-1}}{e^2 + e^1 + e^{-1}} = 0.04$$\n",
    "\n",
    "Esto indica que la clase 1 es la más probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del discriminante lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una red neuronal hay una entrada de datos, seguido de n capas. En cada una de las capas los datos con multiplicados por unos pesos (combinación lineal) y son transformados con base en una función de activación, la cual puede ser diferente o igual a la del resto de capas. Finalmente el modelo arroga un único resultado. Ya que este resultado es bastante malo se distribuye el error calculado en las neuronas a todo el resto de capas, modificando los pesos de la combinación lineal, y mejorando el resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El AUC es una métrica de desempeo que se representa graficamente como el área bajo la curva ROC. Si se toman dos personas al azar una clasificada con 1 y una con 0, el AUC es la probabilidad de que la persona clasificada con 1 tena una mayor probabilidad a la persona clasificada con 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
