{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qué es?\n",
    "\n",
    "La regresión logística es un modelo de clasificación binaria en el que se calcula la probabilidad de ocurrencia. Con base en la probabilidad calculada el modelo discrimina la información de modo que la etiqueta como 1 o 0. Si $p < 0.5$ se clasifica como 0 y si $p > 0.5$ se clasifica como 1.\n",
    "\n",
    "## Cómo se calcula?\n",
    "\n",
    "Para calcular la probabilidad se utiliza el concepto de log odds, con el que se calcula el valor de z, el cual finalmente funciona para el cálcula de las probabilidades. \n",
    "\n",
    "## Verosimilitud General\n",
    "\n",
    "La función de verosimilitud es:\n",
    "\n",
    "$$L(\\theta, \\sigma^2) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Se aplica logaritmo\n",
    "\n",
    "$$\\log L(\\theta, \\sigma^2) = \\sum_{i=1}^{m} \\left( -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Para maximizar la verosilimitud se ignoran los términos constantes\n",
    "\n",
    "$$\\max_{\\theta} \\sum_{i=1}^{m} -\\frac{(y_i - \\theta^T X_i)^2}{2\\sigma^2}$$\n",
    "\n",
    "Esto equivale a minimizar el error cuadrático:\n",
    "\n",
    "$$\\min_{\\theta} \\sum_{i=1}^{m} (y_i - \\theta^T X_i)^2$$\n",
    "\n",
    "\n",
    "Minimizar el error cuadrático con OLS (mínimos cuadrados) es lo mismo que maximizar la verosimilitud.\n",
    "\n",
    "---\n",
    "\n",
    "## Verosimilitud en regresión logaritmica\n",
    "\n",
    "Ya que sigue una distribución de bernoulli, la forma de verosimilitud es:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{m} p(y_i | X_i; \\theta) = \\prod_{i=1}^{m} \\left[ \\sigma(\\theta^T X_i) \\right]^{y_i} \\cdot \\left[ 1 - \\sigma(\\theta^T X_i) \\right]^{(1 - y_i)}$$\n",
    "\n",
    "Al tomar el logaritmo de la verosimilitud se puede cambiar la multiplicatoria por sumatoria, por propiedades de los logaritmos:\n",
    "\n",
    "$$\\log L(\\theta) = \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]$$\n",
    "\n",
    "Maximizamos la log-verosimilitud para estimar $\\theta$. \n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(\\theta^T X_i) + (1 - y_i) \\log (1 - \\sigma(\\theta^T X_i)) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Odds y log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las odds son la razón de éxito frente al fracaso en donde se calculan con la fórmmula:\n",
    "\n",
    "$$\\text{odds} = \\frac{p}{1-p}$$\n",
    "\n",
    "Ya que este crecimiento es exponencial, se aplica logaritmo, de modo que se linealice este comportamiento.\n",
    "\n",
    "$$log \\text{ odds} = log (\\frac{p}{1-p})$$\n",
    "\n",
    "---\n",
    "\n",
    "Una vez que se tienen los coeficientes de la regresión logística, con ayuda del simoide se calcula la combinación lineal de los coeficientes con las características (X), obteniendo el valor de z \n",
    "\n",
    "$$z=\\Theta ^T X$$\n",
    "\n",
    "Con esto se ultiliza la fórmula\n",
    "\n",
    "$$p= \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "donde p es la probabilidad de que el modelo clasifique como 1. Si la probabilidad es menor a 0.5 se clasifica como 0 y si es mayor a 0.5 de clasiifica como 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El softmax se utiliza de manera similar a la regresión logísica, sin embargo, este clasifica en más de dos clases (0 y 1) modelando las probabilidad para cada una de las clases.\n",
    "\n",
    "La función Softmax convierte las salidas de la función lineal en probabilidades:\n",
    "\n",
    "$$P(y = k | \\mathbf{x}) = \\frac{e^{\\mathbf{w}_k \\cdot \\mathbf{x} + b_k}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j \\cdot \\mathbf{x} + b_j}}$$\n",
    "\n",
    "**Ejemplo Numérico**\n",
    "\n",
    "Si tenemos 3 clases y logits calculados como:\n",
    "\n",
    "$$z_1 = 2, \\quad z_2 = 1, \\quad z_3 = -1$$\n",
    "\n",
    "Aplicamos Softmax:\n",
    "\n",
    "$$P(y=1) = \\frac{e^2}{e^2 + e^1 + e^{-1}} = 0.72$$\n",
    "\n",
    "$$P(y=2) = \\frac{e^1}{e^2 + e^1 + e^{-1}} = 0.26$$\n",
    "\n",
    "$$P(y=3) = \\frac{e^{-1}}{e^2 + e^1 + e^{-1}} = 0.04$$\n",
    "\n",
    "Esto indica que la clase 1 es la más probable, por lo que el modelo la clasificará como 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del discriminante lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponiedo que la distribución de cada clase en el modelo es normal multivariada, su forma es:\n",
    "\n",
    "$$P(\\mathbf{x} | y = k) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}} e^{\\left(-\\frac{1}{2}(\\mathbf{x} - \\mu_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k)\\right)}$$\n",
    "\n",
    "Con base en esto, en primer lugar se calcula la proporción de datos pertenecientes por clase, para después calcular la probabilidad de pertenecia de un nuevo dato a cada clase con el teorema de Bayes. El modelo discrimina con la clase cuya probabilidad sea más alta, con la fórmula:\n",
    "\n",
    "$$P(y = k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | y = k) P(y = k)}{P(\\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red neuronal se divide en 2 grandes pasos, forward propagation y back propagation.\n",
    "- **Forward propagation**:\n",
    "     Hay una entrada de datos, seguido de n capas. En cada una de las capas los datos son multiplicados por unos pesos (combinación lineal) y son transformados con base en una función de activación, la cual puede ser diferente o igual a la del resto de capas. Finalmente el modelo arroga un único resultado, el cuál es la combinación lineal de cada una de la capas. \n",
    "     \n",
    "- **Backpropagation**:\n",
    "     Ya que el resultado del forward propagation es bastante malo, se distribuye el error calculado por neurona, a todo el resto de capas. La forma de distribuir este error es modificando los pesos de la combinación lineal de modo que se mejora el resultado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El AUC es una métrica de desempeo que se representa graficamente como el área bajo la curva ROC. Si se toman dos personas al azar, una clasificada con 1 y una con 0, el AUC es la probabilidad de que la persona clasificada con 1 tenga una mayor probabilidad de predicción con el modelo, a la persona clasificada con 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
